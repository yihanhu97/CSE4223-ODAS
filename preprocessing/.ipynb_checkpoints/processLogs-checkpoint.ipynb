{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re \n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time information of each recording from the log file\n",
    "def timeExtract(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Start counting from the last byte\n",
    "        counter = 1\n",
    "        # Go to the 2nd byte before the end of the last line\n",
    "        f.seek(-2, 2) \n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "            counter=counter+1\n",
    "        endTime_line = f.readline().decode()\n",
    "        # Go to the 2nd byte before the end of the last second line\n",
    "        f.seek(-counter-2, 2)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "        startTime_line = f.readline().decode()\n",
    "\n",
    "    return [startTime_line, endTime_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate duration of each recording in microseconds\n",
    "def durationinMicroseconds(filename):\n",
    "    startTime = timeExtract(filename)[0].split()[2:]\n",
    "    endTime = timeExtract(filename)[1].split()[2:]\n",
    "    startTimeStr = startTime[0] + ' ' + startTime[1]\n",
    "    endTimeStr = endTime[0] + ' ' + endTime[1]\n",
    "    T1 = datetime.datetime.strptime(startTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    T2 = datetime.datetime.strptime(endTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    delta = T2-T1\n",
    "    duration = delta.seconds*1000000 + delta.microseconds\n",
    "    \n",
    "    return duration, T1, T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDirectionalities(filename, mic_number):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Use repex to store blocks of data into a list\n",
    "    data = re.split('(?<=})\\n(?={)', text) \n",
    "        # Delete the time info from the last data block\n",
    "    tmp = data[-1][:(data[-1].rfind(\"}\")+1)]\n",
    "    data[-1] = tmp\n",
    "        \n",
    "    #list of src blocks \n",
    "\n",
    "    srcList = [json.loads(block)[\"src\"] for block in data]\n",
    "\n",
    "    \n",
    "    #initialize dataframe to have colums: timestamp, time, data inside source\n",
    "    #timestamp is the initial time stamp\n",
    "    #time is the datetime value converted from the timestamp and intitial time\n",
    "    #source is a 4 by 6 array where the rows are the source, and the columns are the source values\n",
    "    df = pd.DataFrame(columns = ['Timestamp', 'Time', 'Time In Seconds', 'Microphone Number', 'Source ID', 'X', 'Y', 'Z', 'Activity'])\n",
    "    \n",
    "    #Used for calculating timestamps -> time\n",
    "    duration, startTime, endTime = durationinMicroseconds(filename)\n",
    "    start_time_in_seconds = time.mktime(startTime.timetuple())\n",
    "    t = duration/len(data) / 1000000\n",
    "    \n",
    "    index = 1.0\n",
    "    ind = 0\n",
    "    df_dict = {}\n",
    "    for block in srcList:\n",
    "        if block[0][\"id\"] != 0 or block[1][\"id\"] != 0 or block[2][\"id\"] != 0 or block[3][\"id\"] != 0:\n",
    "            time_in_seconds = start_time_in_seconds + (index - 1.0) * t\n",
    "            for i in range(0, 4):\n",
    "                if block[i]['id'] != 0:\n",
    "                    df_dict[ind] = {\"Timestamp\": [index], \"Time\":datetime.datetime.fromtimestamp(time_in_seconds).strftime(\"%A, %B %d, %Y %I:%M:%S\"), \"Time In Seconds\": time_in_seconds, \"Microphone Number\":mic_number, \"Source ID\": block[i][\"id\"], \"X\": block[i][\"x\"], \"Y\": block[i][\"y\"], \"Z\": block[i][\"z\"], \"Activity\": block[i][\"activity\"]}\n",
    "                    ind = ind + 1\n",
    "                    #df = df.append(pd.DataFrame({\"Timestamp\": [index], \"Time\":datetime.datetime.fromtimestamp(time_in_seconds).strftime(\"%A, %B %d, %Y %I:%M:%S\"), \"Time In Seconds\": time_in_seconds, \"Microphone Number\":mic_number, \"Source ID\": block[i][\"id\"], \"X\": block[i][\"x\"], \"Y\": block[i][\"y\"], \"Z\": block[i][\"z\"], \"Activity\": block[i][\"activity\"]}, index=[0]))\n",
    "        index = index + 1.0\n",
    "    \n",
    "    df = df.append(pd.DataFrame.from_dict(df_dict,\"index\"))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 92/9965 [01:54<2:36:48,  1.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: /home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-14/07\n",
      "/home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-14/07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardelalegre/.local/lib/python3.5/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "  1%|▏         | 139/9965 [03:03<4:03:17,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-13/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 150/9965 [03:31<2:52:47,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: /home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-13/20\n",
      "/home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 178/9965 [04:13<5:22:55,  1.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: /h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 242/9965 [05:19<2:49:20,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: /home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-12/09\n",
      "/home/ardelalegre/google-drive/ODAS/dataframes/dataframes0/2020-01-12/09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 657/9965 [13:18<2:32:24,  1.02it/s] "
     ]
    }
   ],
   "source": [
    "# records0 = glob.glob('/home/ardelalegre/google-drive/ODAS/recordings0/*.log')\n",
    "# records1 = glob.glob('/home/ardelalegre/google-drive/ODAS/recordings1/*.log')\n",
    "records2 = glob.glob('/home/ardelalegre/google-drive/ODAS/recordings2/*.log')\n",
    "records3 = glob.glob('/home/ardelalegre/google-drive/ODAS/recordings3/*.log')\n",
    "\n",
    "records = [records2, records3]\n",
    "\n",
    "for mic_number in range(len(records)):\n",
    "\n",
    "    for log in tqdm(records[mic_number]):\n",
    "        \n",
    "        with open(log, 'r') as f:\n",
    "                firstline = f.readline()\n",
    "                if firstline == \"SST log contains no useful data\\n\":\n",
    "                    continue\n",
    "        \n",
    "        hour = log[log.rfind('_') + 1: log.rfind('_') + 1 + 2]\n",
    "        day = log[log.find('_') + 1: log.rfind('_')]\n",
    "        path = log[:log.find('S/') + 2] + 'dataframes/dataframes' + str(mic_number) + '/'\n",
    "        \n",
    "        try:\n",
    "            if(not os.path.isdir(os.path.join(path, day))):\n",
    "                os.mkdir(os.path.join(path, day))\n",
    "            \n",
    "            path = os.path.join(path, day, hour)\n",
    "            if(not os.path.isdir(path)):\n",
    "                print(\"making directory: \" + str(path))\n",
    "                os.mkdir(path)\n",
    "        \n",
    "            file = path+ '/' + log[log.find('_') + 1:log.find('.')]+'.csv'\n",
    "            if(os.path.isfile(file)):\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        print(path)\n",
    "        try:\n",
    "            df = extractDirectionalities(log, mic_number)\n",
    "            if(not os.path.isdir(path)):\n",
    "                os.mkdir(path)\n",
    "            df.to_csv(path_or_buf=path+ '/' + log[log.find('_') + 1:log.find('.')]+'.csv', index=False)\n",
    "        except:\n",
    "            print('Could not process file: ' + log)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
