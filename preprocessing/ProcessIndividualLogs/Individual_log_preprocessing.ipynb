{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: log file name\n",
    "# output: data with plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandasToMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = glob.glob('\"/home/ardelalegre/google-drive/ODAS/logs*/SST/.log\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cSST_2020-07-17_17:20\n"
     ]
    }
   ],
   "source": [
    "# enter date in yyyy-mm-dd format as a string\n",
    "# enter time in hh:mm, where mm is a multiple of 5\n",
    "date = '2020-07-17'\n",
    "hour = '17:20'\n",
    "master_path = 'cSST_' + date + '_' + hour\n",
    "print(master_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of all the log files to be processed\n",
    "a = glob.glob('/home/ardelalegre/google-drive/ODAS/logs[0-3]/SST/' + master_path+'*.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('/home/ardelalegre/google-drive/ODAS/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/home/ardelalegre/google-drive/ODAS/logs0/SST/cSST_2020-07-17_17:20:04_0.log'], ['/home/ardelalegre/google-drive/ODAS/logs1/SST/cSST_2020-07-17_17:20:04_1.log'], ['/home/ardelalegre/google-drive/ODAS/logs2/SST/cSST_2020-07-17_17:20:04_2.log'], ['/home/ardelalegre/google-drive/ODAS/logs3/SST/cSST_2020-07-17_17:20:04_3.log']]\n"
     ]
    }
   ],
   "source": [
    "# converts file names into correct format (list of lists) so that we can just pass this to process logs\n",
    "\n",
    "records = [[],[],[],[]]\n",
    "for i in a:\n",
    "    rec_number = int(i[40])\n",
    "    records[rec_number].append(i)\n",
    "print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re \n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# Extract time information of each recording from the log file\n",
    "def timeExtract(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Start counting from the last byte\n",
    "        counter = 1\n",
    "        # Go to the 2nd byte before the end of the last line\n",
    "        f.seek(-2, 2) \n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "            counter=counter+1\n",
    "        endTime_line = f.readline().decode()\n",
    "        # Go to the 2nd byte before the end of the last second line\n",
    "        f.seek(-counter-2, 2)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "        startTime_line = f.readline().decode()\n",
    "\n",
    "    return [startTime_line, endTime_line]\n",
    "\n",
    "# Calculate duration of each recording in microseconds\n",
    "def durationinMicroseconds(filename):\n",
    "    startTime = timeExtract(filename)[0].split()[2:]\n",
    "    endTime = timeExtract(filename)[1].split()[2:]\n",
    "    startTimeStr = startTime[0] + ' ' + startTime[1]\n",
    "    endTimeStr = endTime[0] + ' ' + endTime[1]\n",
    "    T1 = datetime.datetime.strptime(startTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    T2 = datetime.datetime.strptime(endTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    delta = T2-T1\n",
    "    duration = delta.seconds*1000000 + delta.microseconds\n",
    "    \n",
    "    return duration, T1, T2\n",
    "\n",
    "# Converts .log files into pandas dataframes\n",
    "def extractDirectionalities(filename, mic_number):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Use repex to store blocks of data into a list\n",
    "    data = re.split('(?<=})\\n(?={)', text) \n",
    "        # Delete the time info from the last data block\n",
    "    tmp = data[-1][:(data[-1].rfind(\"}\")+1)]\n",
    "    data[-1] = tmp\n",
    "        \n",
    "    #list of src blocks \n",
    "\n",
    "    srcList = [json.loads(block)[\"src\"] for block in data]\n",
    "\n",
    "    \n",
    "    #initialize dataframe to have colums: timestamp, time, data inside source\n",
    "    #timestamp is the initial time stamp\n",
    "    #time is the datetime value converted from the timestamp and intitial time\n",
    "    #source is a 4 by 6 array where the rows are the source, and the columns are the source values\n",
    "    df = pd.DataFrame(columns = ['Timestamp', 'Time', 'Time In Seconds', 'Microphone Number', 'Source ID', 'X', 'Y', 'Z', 'Activity'])\n",
    "    \n",
    "    #Used for calculating timestamps -> time\n",
    "    duration, startTime, endTime = durationinMicroseconds(filename)\n",
    "    start_time_in_seconds = time.mktime(startTime.timetuple())\n",
    "    t = duration/len(data) / 1000000.0\n",
    "    \n",
    "    index = 1.0\n",
    "    ind = 0\n",
    "    df_dict = {}\n",
    "    for block in srcList:\n",
    "        if block[0][\"id\"] != 0 or block[1][\"id\"] != 0 or block[2][\"id\"] != 0 or block[3][\"id\"] != 0:\n",
    "            time_in_seconds = start_time_in_seconds + (index - 1.0) * t\n",
    "            for i in range(0, 4):\n",
    "                if block[i]['id'] != 0:\n",
    "                    df_dict[ind] = {\"Timestamp\": [index], \"Time\":datetime.datetime.fromtimestamp(time_in_seconds).strftime(\"%A, %B %d, %Y %I:%M:%S\"), \"Time In Seconds\": time_in_seconds, \"Microphone Number\":mic_number, \"Source ID\": block[i][\"id\"], \"X\": block[i][\"x\"], \"Y\": block[i][\"y\"], \"Z\": block[i][\"z\"], \"Activity\": block[i][\"activity\"]}\n",
    "                    ind = ind + 1\n",
    "                    #df = df.append(pd.DataFrame({\"Timestamp\": [index], \"Time\":datetime.datetime.fromtimestamp(time_in_seconds).strftime(\"%A, %B %d, %Y %I:%M:%S\"), \"Time In Seconds\": time_in_seconds, \"Microphone Number\":mic_number, \"Source ID\": block[i][\"id\"], \"X\": block[i][\"x\"], \"Y\": block[i][\"y\"], \"Z\": block[i][\"z\"], \"Activity\": block[i][\"activity\"]}, index=[0]))\n",
    "        index = index + 1.0\n",
    "    \n",
    "    df = df.append(pd.DataFrame.from_dict(df_dict,\"index\"))\n",
    "    return(df)\n",
    "\n",
    "#Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_processor(records):\n",
    "    \n",
    "    # if all elements of records are empty, print (\"there are no records to process\")\n",
    "    # use or operation\n",
    "    if(not(records[0] or records[1] or records[2] or records[3])):\n",
    "        print(\"There are no records to process.\")\n",
    "        return -1\n",
    "    \n",
    "    print(\"Execution of processAllLogs_new.py started at \" + str(datetime.datetime.now()))\n",
    "\n",
    "    # records0 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs0/SST/*.log')\n",
    "    # records1 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs1/SST/*.log')\n",
    "    # records2 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs2/SST/*.log')\n",
    "    # records3 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs3/SST/*.log')\n",
    "\n",
    "    # records = [records0, records1, records2, records3]\n",
    "\n",
    "    for mic_number in range(len(records)):\n",
    "\n",
    "        destination = \"/home/ardelalegre/google-drive/ODAS/logs\" + str(mic_number) + \"/SST/Processed\"\n",
    "        print(\"Now processing logs\" + str(mic_number))\n",
    "\n",
    "        for log in records[mic_number]:\n",
    "\n",
    "            with open(log, 'r') as f:\n",
    "                    firstline = f.readline()\n",
    "                    if firstline == \"SST log contains no useful data\\n\":\n",
    "                        try:\n",
    "                            temp = shutil.move(log,destination) # new\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        continue\n",
    "\n",
    "            log_string = log[:-6] + '.log' # modification to account for added array number to path\n",
    "    #         print(log_string)\n",
    "\n",
    "            hour = log_string[log_string.rfind('_') + 1: log_string.rfind('_') + 1 + 2]\n",
    "            day = log_string[log_string.find('_') + 1: log_string.rfind('_')]\n",
    "            path = log_string[:log_string.find('S/') + 2] + 'dataframes/dataframes' + str(mic_number) + '/'\n",
    "    #         print(path)\n",
    "    #         print(day)\n",
    "    #         print(hour)\n",
    "            try:\n",
    "                if(not os.path.isdir(os.path.join(path, day))):\n",
    "                    os.mkdir(os.path.join(path, day))\n",
    "\n",
    "                path = os.path.join(path, day, hour)\n",
    "                if(not os.path.isdir(path)):\n",
    "                    print(\"making directory: \" + str(path))\n",
    "                    os.mkdir(path)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    #         print(path)\n",
    "            try:\n",
    "                df = extractDirectionalities(log, mic_number)\n",
    "                if(not os.path.isdir(path)):\n",
    "                    os.mkdir(path)\n",
    "                df.to_csv(path_or_buf=path+ '/' + log[log.find('_') + 1:log.find('.')]+'.csv', index=False)\n",
    "                temp = shutil.move(log,destination) # new\n",
    "            except:\n",
    "                print('Could not process file: ' + log)\n",
    "\n",
    "\n",
    "    print(\"Execution of processAllLogs_new.py ended at \" + str(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_processor_and_merger(records):\n",
    "    \n",
    "    dfs = []\n",
    "    # if all elements of records are empty, print (\"there are no records to process\")\n",
    "    # use or operation\n",
    "    if(not(records[0] or records[1] or records[2] or records[3])):\n",
    "        print(\"There are no records to process.\")\n",
    "        return -1\n",
    "    \n",
    "\n",
    "    # records0 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs0/SST/*.log')\n",
    "    # records1 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs1/SST/*.log')\n",
    "    # records2 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs2/SST/*.log')\n",
    "    # records3 = glob.glob('/home/ardelalegre/google-drive/ODAS/logs3/SST/*.log')\n",
    "\n",
    "    # records = [records0, records1, records2, records3]\n",
    "\n",
    "    for mic_number in range(len(records)):\n",
    "\n",
    "#         destination = \"/home/ardelalegre/google-drive/ODAS/logs\" + str(mic_number) + \"/SST/Processed\"\n",
    "        print(\"Now processing logs\" + str(mic_number))\n",
    "\n",
    "        for log in records[mic_number]:\n",
    "\n",
    "            with open(log, 'r') as f:\n",
    "                    firstline = f.readline()\n",
    "                    if firstline == \"SST log contains no useful data\\n\":\n",
    "#                         try:\n",
    "#                             temp = shutil.move(log,destination) # new\n",
    "#                         except:\n",
    "#                             continue\n",
    "\n",
    "                        continue\n",
    "\n",
    "            log_string = log[:-6] + '.log' # modification to account for added array number to path\n",
    "    #         print(log_string)\n",
    "\n",
    "#             hour = log_string[log_string.rfind('_') + 1: log_string.rfind('_') + 1 + 2]\n",
    "#             day = log_string[log_string.find('_') + 1: log_string.rfind('_')]\n",
    "#             path = log_string[:log_string.find('S/') + 2] + 'dataframes/dataframes' + str(mic_number) + '/'\n",
    "    #         print(path)\n",
    "    #         print(day)\n",
    "    #         print(hour)\n",
    "#             try:\n",
    "#                 if(not os.path.isdir(os.path.join(path, day))):\n",
    "#                     os.mkdir(os.path.join(path, day))\n",
    "\n",
    "#                 path = os.path.join(path, day, hour)\n",
    "#                 if(not os.path.isdir(path)):\n",
    "#                     print(\"making directory: \" + str(path))\n",
    "#                     os.mkdir(path)\n",
    "\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "    #         print(path)\n",
    "            try:\n",
    "                df = extractDirectionalities(log, mic_number)\n",
    "                dfs.append(df)\n",
    "#                 if(not os.path.isdir(path)):\n",
    "#                     os.mkdir(path)\n",
    "#                 df.to_csv(path_or_buf=path+ '/' + log[log.find('_') + 1:log.find('.')]+'.csv', index=False)\n",
    "#                 temp = shutil.move(log,destination) # new\n",
    "\n",
    "            except:\n",
    "                print('Could not process file: ' + log)\n",
    "    if(len(dfs) > 0):\n",
    "        merged = pd.concat(dfs)\n",
    "        #sort by time\n",
    "        merged = merged.sort_values(['Time In Seconds'])\n",
    "    list_of_lists_array = pandasToMatrix.source_matching_single(merged)\n",
    "    return(list_of_lists_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing logs0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardelalegre/.local/lib/python3.5/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing logs1\n",
      "Now processing logs2\n",
      "Now processing logs3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardelalegre/CSE4223-ODAS/preprocessing/ProcessIndividualLogs/pandasToMatrix.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  temp['X'] = None\n",
      "/home/ardelalegre/CSE4223-ODAS/preprocessing/ProcessIndividualLogs/pandasToMatrix.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  temp['Y'] = None\n",
      "/home/ardelalegre/CSE4223-ODAS/preprocessing/ProcessIndividualLogs/pandasToMatrix.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  temp['Z'] = None\n",
      "/home/ardelalegre/CSE4223-ODAS/preprocessing/ProcessIndividualLogs/pandasToMatrix.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  temp['Microphone Number'] = None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-81d3160a154d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlol_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_processor_and_merger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c7422931f0cb>\u001b[0m in \u001b[0;36mlog_processor_and_merger\u001b[0;34m(records)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#sort by time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Time In Seconds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mlist_of_lists_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasToMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_matching_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_lists_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSE4223-ODAS/preprocessing/ProcessIndividualLogs/pandasToMatrix.py\u001b[0m in \u001b[0;36msource_matching_single\u001b[0;34m(dfs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Time In Seconds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m#             print('ok')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdata_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m13\u001b[0m           \u001b[0;31m# list with 13 None values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1477\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "lol_data = log_processor_and_merger(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
