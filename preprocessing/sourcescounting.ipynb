{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\"File\":[], \n",
    "           \"Start Time\":[], \n",
    "           \"End Time\":[],\n",
    "           \"Timestamp Length(ms)\":[],\n",
    "           \"Total Timestamps\":[],\n",
    "           \"0 source detected\":[],\n",
    "           \"1 source detected\":[],\n",
    "           \"2 source detected\":[],\n",
    "           \"3 source detected\":[],\n",
    "           \"4 source detected\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time information of each recording from the log file\n",
    "def timeExtract(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Start counting from the last byte\n",
    "        counter = 1\n",
    "        # Go to the 2nd byte before the end of the last line\n",
    "        f.seek(-2, 2) \n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "            counter=counter+1\n",
    "        endTime_line = f.readline().decode()\n",
    "        # Go to the 2nd byte before the end of the last second line\n",
    "        f.seek(-counter-2, 2)\n",
    "        while f.read(1) != b'\\n':\n",
    "            f.seek(-2, 1)\n",
    "        startTime_line = f.readline().decode()\n",
    "\n",
    "    return [startTime_line, endTime_line]\n",
    "\n",
    "# Calculate duration of each recording in microseconds\n",
    "def durationinMicroseconds(filename):\n",
    "    startTime = timeExtract(filename)[0].split()[2:]\n",
    "    endTime = timeExtract(filename)[1].split()[2:]\n",
    "    startTimeStr = startTime[0] + ' ' + startTime[1]\n",
    "    endTimeStr = endTime[0] + ' ' + endTime[1]\n",
    "    T1 = datetime.datetime.strptime(startTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    T2 = datetime.datetime.strptime(endTimeStr, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    delta = T2-T1\n",
    "    duration = delta.seconds*1000000 + delta.microseconds\n",
    "    \n",
    "    return duration, T1, T2\n",
    "\n",
    "def appendSummary(summary,filename,startTime,endTime,t,active):\n",
    "    summary[\"File\"].append(filename.split('/')[-1])\n",
    "    summary[\"Start Time\"].append(str(startTime))\n",
    "    summary[\"End Time\"].append(str(endTime))\n",
    "    summary[\"Timestamp Length(ms)\"].append(round(t/1000,4))\n",
    "    summary[\"Total Timestamps\"].append(len(active))\n",
    "    summary[\"0 source detected\"].append(active.count(0))\n",
    "    summary[\"1 source detected\"].append(active.count(1))\n",
    "    summary[\"2 source detected\"].append(active.count(2))\n",
    "    summary[\"3 source detected\"].append(active.count(3))\n",
    "    summary[\"4 source detected\"].append(active.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"/Users/hyh/Google Drive/ODAS/recordings3/*.log\"):\n",
    "    with open(filename, 'r') as f:\n",
    "        firstline = f.readline()\n",
    "    if firstline == \"SST log contains no useful data\\n\":\n",
    "        pass\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "        # Use repex to store blocks of data into a list\n",
    "        data = re.split('(?<=})\\n(?={)', text) \n",
    "        # Delete the time info from the last data block\n",
    "        tmp = data[-1][:(data[-1].rfind(\"}\")+1)]\n",
    "        data[-1] = tmp\n",
    "\n",
    "        # Obtain duration, start time, end time\n",
    "        duration, startTime, endTime = durationinMicroseconds(filename)\n",
    "        # Calculate how fast each timestamp is printed\n",
    "        t = duration/len(data)\n",
    "        # Extract src keys from all blocks and wrap them in a list\n",
    "        srcList = [json.loads(block)[\"src\"] for block in data]\n",
    "\n",
    "        # def foo(src):\n",
    "        #     return [line[\"id\"] for line in src]\n",
    "        foo = lambda src: [line[\"id\"] for line in src]\n",
    "        # Extract id values from all src keys and wrap them in a list\n",
    "        idList = [foo(src) for src in srcList]\n",
    "        # Count the number of active sources at each timestamp and wrap them in a list\n",
    "        active = [4-row.count(0) for row in idList]\n",
    "        \n",
    "        appendSummary(summary,filename,startTime,endTime,t,active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary3 = pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary3.to_csv('/Users/hyh/Dev/Freund Research Project/ODAS/Files/CSV/'+'ODAS3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import hashlib\n",
    "\n",
    "\n",
    "# def chunk_reader(fobj, chunk_size=1024):\n",
    "#     \"\"\"Generator that reads a file in chunks of bytes\"\"\"\n",
    "#     while True:\n",
    "#         chunk = fobj.read(chunk_size)\n",
    "#         if not chunk:\n",
    "#             return\n",
    "#         yield chunk\n",
    "\n",
    "\n",
    "# def get_hash(filename, first_chunk_only=False, hash=hashlib.sha1):\n",
    "#     hashobj = hash()\n",
    "#     file_object = open(filename, 'rb')\n",
    "\n",
    "#     if first_chunk_only:\n",
    "#         hashobj.update(file_object.read(1024))\n",
    "#     else:\n",
    "#         for chunk in chunk_reader(file_object):\n",
    "#             hashobj.update(chunk)\n",
    "#     hashed = hashobj.digest()\n",
    "\n",
    "#     file_object.close()\n",
    "#     return hashed\n",
    "\n",
    "\n",
    "# def check_for_duplicates(paths, hash=hashlib.sha1):\n",
    "#     hashes_by_size = {}\n",
    "#     hashes_on_1k = {}\n",
    "#     hashes_full = {}\n",
    "\n",
    "#     for path in paths:\n",
    "#         for dirpath, dirnames, filenames in os.walk(path):\n",
    "#             for filename in filenames:\n",
    "#                 full_path = os.path.join(dirpath, filename)\n",
    "#                 try:\n",
    "#                     # if the target is a symlink (soft one), this will \n",
    "#                     # dereference it - change the value to the actual target file\n",
    "#                     full_path = os.path.realpath(full_path)\n",
    "#                     file_size = os.path.getsize(full_path)\n",
    "#                 except (OSError,):\n",
    "#                     # not accessible (permissions, etc) - pass on\n",
    "#                     continue\n",
    "\n",
    "#                 duplicate = hashes_by_size.get(file_size)\n",
    "\n",
    "#                 if duplicate:\n",
    "#                     hashes_by_size[file_size].append(full_path)\n",
    "#                 else:\n",
    "#                     hashes_by_size[file_size] = []  # create the list for this file size\n",
    "#                     hashes_by_size[file_size].append(full_path)\n",
    "\n",
    "#     # For all files with the same file size, get their hash on the 1st 1024 bytes\n",
    "#     for __, files in hashes_by_size.items():\n",
    "#         if len(files) < 2:\n",
    "#             continue    # this file size is unique, no need to spend cpy cycles on it\n",
    "\n",
    "#         for filename in files:\n",
    "#             try:\n",
    "#                 small_hash = get_hash(filename, first_chunk_only=True)\n",
    "#             except (OSError,):\n",
    "#                 # the file access might've changed till the exec point got here \n",
    "#                 continue\n",
    "\n",
    "#             duplicate = hashes_on_1k.get(small_hash)\n",
    "#             if duplicate:\n",
    "#                 hashes_on_1k[small_hash].append(filename)\n",
    "#             else:\n",
    "#                 hashes_on_1k[small_hash] = []          # create the list for this 1k hash\n",
    "#                 hashes_on_1k[small_hash].append(filename)\n",
    "\n",
    "#     # For all files with the hash on the 1st 1024 bytes, get their hash on the full file - collisions will be duplicates\n",
    "#     for __, files in hashes_on_1k.items():\n",
    "#         if len(files) < 2:\n",
    "#             continue    # this hash of fist 1k file bytes is unique, no need to spend cpy cycles on it\n",
    "\n",
    "#         for filename in files:\n",
    "#             try: \n",
    "#                 full_hash = get_hash(filename, first_chunk_only=False)\n",
    "#             except (OSError,):\n",
    "#                 # the file access might've changed till the exec point got here \n",
    "#                 continue\n",
    "\n",
    "#             duplicate = hashes_full.get(full_hash)\n",
    "#             if duplicate:\n",
    "#                 print (\"Duplicate found: %s and %s\" % (filename, duplicate))\n",
    "#             else:\n",
    "#                 hashes_full[full_hash] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
