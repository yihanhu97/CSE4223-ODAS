{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is need of a script that:\n",
    "- Runs often (daily)\n",
    "- Uses pandas to Matrix to convert to list of lists\n",
    "- converts the list of lists to a numpy array\n",
    "- pushes the numpy array to SQL\n",
    "\n",
    "We have all the requisite functions already. It just needs to be combined in order to form one script which does everything. We will try to create such a script using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandasToMatrix\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-07-23'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose a date first. We will choose July 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ardelalegre/google-drive/ODAS/dataframes/combined/2020-07-23'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c91f07e199d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_with_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasToMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ece299/newece299/CSE4223-ODAS/preprocessing/SQL Project/pandasToMatrix.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(date, starting_hour, num_hours, multiple_sources)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# right now, we are just listing all the hours for which a combined .csv exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# non_contiguous_hours returns a list of integers corresponding to such hours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mhours_ints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_contiguous_hours_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_path_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece299/newece299/CSE4223-ODAS/preprocessing/SQL Project/pandasToMatrix.py\u001b[0m in \u001b[0;36mnon_contiguous_hours_extractor\u001b[0;34m(date, path)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnon_contiguous_hours_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;34m\"\"\"Returns an integer list of all hours for which a .csv exists. The path needs to have only hh.csv files\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mlist_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mlist_dir_nums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ardelalegre/google-drive/ODAS/dataframes/combined/2020-07-23'"
     ]
    }
   ],
   "source": [
    "data_with_time = pandasToMatrix.main(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_array(data_with_time_array):\n",
    "    data = np.zeros((data_with_time_array.shape[0],13))\n",
    "    for (index,i) in enumerate(data_with_time_array):\n",
    "        data[index] = np.asarray(data_with_time_array[index])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_np_arr(list_form):\n",
    "    \"\"\"Converts list of lists into numpy array.\"\"\"\n",
    "    data = np.zeros((1,13))\n",
    "    for i in list_form:\n",
    "        data = np.append(data,np.asarray(i), axis = 0)\n",
    "    data = data[1:]\n",
    "    data = nested_list_to_array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_time_array = list_to_np_arr(data_with_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_df_single_row(index):\n",
    "    dt = datetime.datetime.fromtimestamp(data_with_time_array[index,12])\n",
    "    single_row = [\n",
    "        {'timestamp' : data_with_time_array[index,12], 'ar0_x' : data_with_time_array[index,0], 'ar0_y' : data_with_time_array[index,1],'ar0_z' : data_with_time_array[index,2], 'ar1_x' : data_with_time_array[index,3], 'ar1_y' : data_with_time_array[index,4],'ar1_z' : data_with_time_array[index,5], 'ar2_x' : data_with_time_array[index,6], 'ar2_y' : data_with_time_array[index,7],'ar2_z' : data_with_time_array[index,8], 'ar3_x' : data_with_time_array[index,9], 'ar3_y' : data_with_time_array[index,10],'ar3_z' : data_with_time_array[index,11], 'Year' : dt.year, 'Month': dt.month, 'Date':dt.day, 'Hour': dt.hour, 'Minute':dt.minute, 'Second':dt.second}\n",
    "    ]\n",
    "    df = pd.DataFrame(single_row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_faster_append = []\n",
    "for i in range(data_with_time_array.shape[0]):\n",
    "    list_for_faster_append.append(np_to_df_single_row(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(list_for_faster_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sqlalchemy engine\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"root\",\n",
    "                               pw=\"rootpasswordgiven\",\n",
    "                               db=\"hello_world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('odas_data', con = engine, if_exists = 'append', chunksize = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
